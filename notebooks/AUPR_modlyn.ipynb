{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56dcc3-c222-4483-9347-8ba201579c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import anndata as ad\n",
    "import lightning as L\n",
    "import lamindb as ln\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch\n",
    "\n",
    "from modlyn.io.loading import read_lazy\n",
    "from modlyn.io.datamodules import ClassificationDataModule\n",
    "from modlyn.models.linear import Linear ## should move to modlyn not to arrayloader - cp to folder - maintain API structure - name the folders and sub-modules\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project = ln.Project(name=\"Modlyn\")\n",
    "project.save()\n",
    "\n",
    "ln.track(project=\"Modlyn\")\n",
    "\n",
    "run = ln.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b27f6-4d9c-4e6d-a82b-90725c83e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LAMIN_CACHE_DIR\"] = \"/data/.lamindb-cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4ae33-3398-4ece-bf18-82d6082279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from modlyn.io import read_lazy\n",
    "\n",
    "store_path = Path(\"/data/.lamindb-cache/lamin-us-west-2/wXDsTYYd/tahoe100M_shuffled_zarr_store_2025-05-07/chunk_30.zarr\")\n",
    "adata = read_lazy(store_path)\n",
    "var = pd.read_parquet(\"var_subset_tahoe100M.parquet\")\n",
    "adata.var = var\n",
    "adata.obs[\"y\"] = adata.obs[\"cell_line\"].astype(\"category\").cat.codes.astype(\"i8\")\n",
    "# adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca598b3a-00b9-4618-96eb-b84a9da3cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a7bde-e52a-4725-8a1f-d7850690ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset\n",
    "n = adata.n_obs\n",
    "\n",
    "n_train = int(n * 0.8)\n",
    "n_val = n - n_train\n",
    "\n",
    "adata_train = adata[:n_train]\n",
    "adata_val = adata[n_train:]\n",
    "adata_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cbc3e-a9b6-46f7-8abc-baaf4d20e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTracker(L.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        loss = trainer.callback_metrics[\"train_loss\"]\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        loss = trainer.callback_metrics[\"val_loss\"]\n",
    "        self.val_losses.append(loss.item())\n",
    "\n",
    "datamodule = ClassificationDataModule(\n",
    "    adata_train=adata_train,\n",
    "    adata_val=adata_val,\n",
    "    label_column=\"y\",\n",
    "    train_dataloader_kwargs={\"batch_size\": 2048, \"drop_last\": True},\n",
    "    val_dataloader_kwargs={\"batch_size\": 2048, \"drop_last\": False},\n",
    ")\n",
    "\n",
    "linear = Linear(\n",
    "    n_genes=adata.n_vars,\n",
    "    n_covariates=adata.obs[\"y\"].nunique(),\n",
    "    learning_rate=1e-2,\n",
    ")\n",
    "\n",
    "loss_tracker = LossTracker()\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    max_steps=3000,\n",
    "    log_every_n_steps=100,\n",
    "    callbacks=[loss_tracker]\n",
    ")\n",
    "trainer.fit(linear, datamodule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e174e82-ef3b-459c-b1cf-48879600b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_tracker.train_losses, marker='o', label=\"train_loss\")\n",
    "plt.plot(loss_tracker.val_losses,   marker='x', label=\"val_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c41f5-01d9-4b76-9f52-87677559d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = linear.linear.weight.detach().cpu().numpy()\n",
    "top_cell_lines = adata.obs[\"cell_line\"].value_counts().index[:weights.shape[0]].tolist()\n",
    "\n",
    "weights_df = pd.DataFrame(\n",
    "    weights, \n",
    "    columns=adata.var_names,\n",
    "    index=top_cell_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5046b-8cf1-4c9c-86d5-30f94acdce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "\n",
    "def compute_fisher_info_and_se(model, dataloader):\n",
    "    model.eval()\n",
    "    fisher_diag = torch.zeros_like(model.linear.weight)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Computing Fisher Information\"):\n",
    "            x, y = batch\n",
    "            logits = model.linear(x)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range(probs.shape[1]):\n",
    "                p = probs[:, i].unsqueeze(1)\n",
    "                fisher_i = p * (1 - p) * x**2\n",
    "                fisher_diag[i] += fisher_i.sum(dim=0)\n",
    "\n",
    "    se = torch.sqrt(1.0 / (fisher_diag + 1e-8))  # Add epsilon for numerical stability\n",
    "    confidence = 1.0 / se**2\n",
    "    return se.cpu().numpy(), confidence.cpu().numpy()\n",
    "\n",
    "se, confidence = compute_fisher_info_and_se(linear, datamodule.val_dataloader())\n",
    "confidence_df = pd.DataFrame(\n",
    "    confidence,\n",
    "    columns=adata.var_names,\n",
    "    index=weights_df.index  # real cell line names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ed586-3272-4056-b36c-b89073d52af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# ─── 1) Load your precomputed AnnData ──────────────────────────────────────────\n",
    "# adata_pre = sc.read_h5ad('adata_chunk30_processed.h5ad')\n",
    "print(adata_pre)\n",
    "# inspect what keys were stored\n",
    "print(adata_pre.uns.keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef3eb7-1cb0-4578-9856-bda3a64d5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.rank_genes_groups_dotplot(\n",
    "    adata_pre,\n",
    "    groupby='cell_line',\n",
    "    key='logreg', \n",
    "    n_genes=10,\n",
    "    title='Precomputed Scanpy LogReg (Top 10)'\n",
    ")\n",
    "sc.pl.rank_genes_groups_dotplot(\n",
    "    adata_pre,\n",
    "    groupby='cell_line',\n",
    "    key='wilcoxon',\n",
    "    n_genes=10,\n",
    "    title='Precomputed Scanpy Wilcoxon (Top 10)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27460d-5cc9-43b6-b1a8-09873d7fc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3) Build your model’s dotplot as before ───────────────────────────────────\n",
    "\n",
    "# assume `weights_df` and `confidence_df` are already in memory from your trained Linear\n",
    "# pick top‐10 genes per cell_line from your model\n",
    "n_top = 10\n",
    "top_model = {\n",
    "    cl: weights_df.loc[cl].nlargest(n_top).index.tolist()\n",
    "    for cl in weights_df.index\n",
    "}\n",
    "genes_model = list({g for genes in top_model.values() for g in genes})\n",
    "\n",
    "# scale your confidence for dot size\n",
    "conf_sub = confidence_df.loc[weights_df.index, genes_model]\n",
    "conf_clipped = np.clip(conf_sub, 0, np.percentile(conf_sub.values, 99))\n",
    "conf_scaled = pd.DataFrame(\n",
    "    minmax_scale(conf_clipped, axis=1),\n",
    "    index=conf_clipped.index,\n",
    "    columns=conf_clipped.columns\n",
    ")\n",
    "\n",
    "# build AnnData for your model\n",
    "adata_model = sc.AnnData(\n",
    "    X=conf_scaled.values,\n",
    "    obs=pd.DataFrame(index=conf_scaled.index),\n",
    "    var=pd.DataFrame(index=conf_scaled.columns)\n",
    ")\n",
    "adata_model.obs['cell_line'] = adata_model.obs.index\n",
    "adata_model.layers['weights'] = weights_df.loc[\n",
    "    adata_model.obs_names, adata_model.var_names\n",
    "].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aeb02a-9901-4a21-8a95-a16a45a021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f926c-3b2b-4f93-9a92-74f26ef14f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(\n",
    "    adata_model,\n",
    "    var_names=genes_model,\n",
    "    groupby='cell_line',\n",
    "    use_raw=False,\n",
    "    layer='weights',\n",
    "    dot_min=0.05,\n",
    "    dot_max=0.6,\n",
    "    title='modlyn linear: Weight & Confidence'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfffb9-4e3f-416e-8fd3-4e9303b676ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_model.write(\"adata_modlyn_chunk30_trained_with_confidence.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8f982-7ceb-4256-be2b-ec445e49dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_key = 'logreg'  \n",
    "# lr_names = adata_pre.uns['logreg']['names']   # shape: (n_groups, n_genes)\n",
    "# flat = [g for row in lr_names for g in row]   # flatten list of lists\n",
    "# genes = []\n",
    "# for g in flat:\n",
    "#     if g not in genes:\n",
    "#         genes.append(g)\n",
    "#     if len(genes) == 50:\n",
    "#         break\n",
    "\n",
    "# print(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa21f6e-bf8c-468c-840c-3059f5586bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# ─── Assumptions ────────────────────────────────────────────────────────────────\n",
    "# • adata_pre: AnnData with `.uns['logreg']['names']`, `.uns['logreg']['scores']`, \n",
    "#   and similarly for 'wilcoxon'\n",
    "# • adata_model: AnnData with .layers['weights'] and .X = scaled certainty\n",
    "# • genes: list of genes (e.g. top 50 from logreg) you want to compare\n",
    "\n",
    "# ─── 0) Filter genes to those present in the model AnnData ───────────────────────\n",
    "common_genes = [g for g in genes if g in adata_model.var_names]\n",
    "if len(common_genes) < len(genes):\n",
    "    missing = set(genes) - set(common_genes)\n",
    "    print(f\"Warning: dropping {len(missing)} genes not in model: {missing}\")\n",
    "genes = common_genes\n",
    "\n",
    "# ─── 1) Build Scanpy logreg_scores DataFrame ────────────────────────────────────\n",
    "scores_lr = adata_pre.uns['logreg']['scores']\n",
    "names_lr  = adata_pre.uns['logreg']['names']\n",
    "groups    = scores_lr.dtype.names\n",
    "\n",
    "lr_dict = {\n",
    "    cl: pd.Series(scores_lr[cl], index=names_lr[cl])\n",
    "    for cl in groups\n",
    "}\n",
    "logreg_scores = pd.DataFrame(lr_dict).T[genes]\n",
    "\n",
    "# ─── 2) Build Scanpy wilcoxon_scores DataFrame ─────────────────────────────────\n",
    "scores_wl = adata_pre.uns['wilcoxon']['scores']\n",
    "names_wl  = adata_pre.uns['wilcoxon']['names']\n",
    "\n",
    "wl_dict = {\n",
    "    cl: pd.Series(scores_wl[cl], index=names_wl[cl])\n",
    "    for cl in scores_wl.dtype.names\n",
    "}\n",
    "wilcoxon_scores = pd.DataFrame(wl_dict).T[genes]\n",
    "\n",
    "# ─── 3) Extract your model’s weights & certainty ───────────────────────────────\n",
    "modlyn_weights   = pd.DataFrame(\n",
    "    adata_model.layers['weights'],\n",
    "    index=adata_model.obs_names,\n",
    "    columns=adata_model.var_names\n",
    ")[genes]\n",
    "\n",
    "modlyn_certainty = pd.DataFrame(\n",
    "    adata_model.X,\n",
    "    index=adata_model.obs_names,\n",
    "    columns=adata_model.var_names\n",
    ")[genes]\n",
    "\n",
    "# ─── 4) Scale values for dot color ([-1,1]) ─────────────────────────────────────\n",
    "def scale_weights(df):\n",
    "    vmax = np.percentile(np.abs(df.values), 99)\n",
    "    return df.clip(-vmax, vmax) / vmax\n",
    "\n",
    "logreg_scaled   = scale_weights(logreg_scores)\n",
    "wilcoxon_scaled = scale_weights(wilcoxon_scores)\n",
    "modlyn_scaled   = scale_weights(modlyn_weights)\n",
    "\n",
    "# ─── 5) Scale values for dot size ([0,1]) ──────────────────────────────────────\n",
    "logreg_size   = pd.DataFrame(\n",
    "    minmax_scale(logreg_scores.abs(),   axis=1),\n",
    "    index=logreg_scores.index,   columns=genes\n",
    ")\n",
    "wilcoxon_size = pd.DataFrame(\n",
    "    minmax_scale(wilcoxon_scores.abs(), axis=1),\n",
    "    index=wilcoxon_scores.index, columns=genes\n",
    ")\n",
    "modlyn_size   = pd.DataFrame(\n",
    "    minmax_scale(modlyn_certainty,      axis=1),\n",
    "    index=modlyn_certainty.index,      columns=genes\n",
    ")\n",
    "\n",
    "# ─── 6) Plot side-by-side dotplots ─────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sc.pl.dotplot(\n",
    "    adata_pre,\n",
    "    var_names=genes,\n",
    "    groupby='cell_line',\n",
    "    dot_color_df=logreg_scaled,\n",
    "    dot_size_df=logreg_size,\n",
    "    ax=axes[0],\n",
    "    cmap='RdBu_r',\n",
    "    vcenter=0,\n",
    "    dot_min=0.2,\n",
    "    dot_max=1.0,\n",
    "    smallest_dot=0.1,\n",
    "    show=False\n",
    ")\n",
    "axes[0].set_title('Scanpy LogReg (scaled)')\n",
    "\n",
    "sc.pl.dotplot(\n",
    "    adata_pre,\n",
    "    var_names=genes,\n",
    "    groupby='cell_line',\n",
    "    dot_color_df=wilcoxon_scaled,\n",
    "    dot_size_df=wilcoxon_size,\n",
    "    ax=axes[1],\n",
    "    cmap='RdBu_r',\n",
    "    vcenter=0,\n",
    "    dot_min=0.2,\n",
    "    dot_max=1.0,\n",
    "    smallest_dot=0.1,\n",
    "    show=False\n",
    ")\n",
    "axes[1].set_title('Scanpy Wilcoxon (scaled)')\n",
    "\n",
    "sc.pl.dotplot(\n",
    "    adata_model,\n",
    "    var_names=genes,\n",
    "    groupby='cell_line',\n",
    "    dot_color_df=modlyn_scaled,\n",
    "    dot_size_df=modlyn_size,\n",
    "    ax=axes[2],\n",
    "    cmap='RdBu_r',\n",
    "    vcenter=0,\n",
    "    dot_min=0.2,\n",
    "    dot_max=1.0,\n",
    "    smallest_dot=0.1,\n",
    "    show=False\n",
    ")\n",
    "axes[2].set_title('Modlyn (scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13ac52-b7bc-47f3-8c78-a2206d3124dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ─── 1) Assemble the three DataFrames on the same genes & cell_lines ───────────\n",
    "\n",
    "# A) Scanpy logistic regression scores\n",
    "lr_scores = {\n",
    "    cl: pd.Series(adata_pre.uns['logreg']['scores'][cl],\n",
    "                  index=adata_pre.uns['logreg']['names'][cl])\n",
    "    for cl in adata_pre.uns['logreg']['scores'].dtype.names\n",
    "}\n",
    "df_lr = pd.DataFrame(lr_scores).T  # shape: (cell_line × gene)\n",
    "\n",
    "# B) Scanpy Wilcoxon scores\n",
    "wl_scores = {\n",
    "    cl: pd.Series(adata_pre.uns['wilcoxon']['scores'][cl],\n",
    "                  index=adata_pre.uns['wilcoxon']['names'][cl])\n",
    "    for cl in adata_pre.uns['wilcoxon']['scores'].dtype.names\n",
    "}\n",
    "df_wl = pd.DataFrame(wl_scores).T\n",
    "\n",
    "# C) Your model’s raw weights (no Fisher info)\n",
    "df_ml = pd.DataFrame(\n",
    "    adata_model.layers['weights'],\n",
    "    index=adata_model.obs_names,\n",
    "    columns=adata_model.var_names\n",
    ")\n",
    "\n",
    "# D) Restrict to shared genes & sorted cell_lines\n",
    "common_genes = sorted(set(df_lr.columns) & set(df_wl.columns) & set(df_ml.columns))\n",
    "common_cells = sorted(set(df_lr.index) & set(df_wl.index) & set(df_ml.index))\n",
    "\n",
    "df_lr = df_lr.loc[common_cells, common_genes]\n",
    "df_wl = df_wl.loc[common_cells, common_genes]\n",
    "df_ml = df_ml.loc[common_cells, common_genes]\n",
    "\n",
    "# ─── 2) For each cell_line, get top-N gene lists by absolute score/weight ───────\n",
    "N = 50\n",
    "top_lr  = {cl: df_lr.loc[cl].abs().nlargest(N).index.tolist() for cl in common_cells}\n",
    "top_wl  = {cl: df_wl.loc[cl].abs().nlargest(N).index.tolist() for cl in common_cells}\n",
    "top_ml  = {cl: df_ml.loc[cl].abs().nlargest(N).index.tolist() for cl in common_cells}\n",
    "\n",
    "# ─── 3) Compute overlaps and Spearman correlations ──────────────────────────────\n",
    "records = []\n",
    "for cl in common_cells:\n",
    "    set_lr, set_wl, set_ml = set(top_lr[cl]), set(top_wl[cl]), set(top_ml[cl])\n",
    "    \n",
    "    # top-N overlaps\n",
    "    overlap_lr_ml = len(set_lr & set_ml)\n",
    "    overlap_lr_wl = len(set_lr & set_wl)\n",
    "    \n",
    "    # rank correlations on all shared genes\n",
    "    rho_lr_ml = spearmanr(df_lr.loc[cl, common_genes], df_ml.loc[cl, common_genes]).correlation\n",
    "    rho_lr_wl = spearmanr(df_lr.loc[cl, common_genes], df_wl.loc[cl, common_genes]).correlation\n",
    "    \n",
    "    records.append({\n",
    "        'cell_line': cl,\n",
    "        'overlap_logreg_modlyn': overlap_lr_ml,\n",
    "        'overlap_logreg_wilcox': overlap_lr_wl,\n",
    "        'spearman_logreg_modlyn': rho_lr_ml,\n",
    "        'spearman_logreg_wilcox': rho_lr_wl\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(records).set_index('cell_line')\n",
    "# print(comparison_df)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    comparison_df,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"vlag\",\n",
    "    cbar_kws={\"label\": \"Value\"},\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"Comparison of Model vs. Scanpy Metrics per Cell Line\")\n",
    "plt.ylabel(\"Cell Line\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da5c34-10f9-4ff6-98fe-b611acdc888f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e8f62-d93d-473d-a5ef-144f7bc38bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ─── 1) Assemble DataFrames for each method ────────────────────────────────────\n",
    "# A) Scanpy logistic regression scores\n",
    "lr_scores = {\n",
    "    cl: pd.Series(adata_pre.uns['logreg']['scores'][cl],\n",
    "                  index=adata_pre.uns['logreg']['names'][cl])\n",
    "    for cl in adata_pre.uns['logreg']['scores'].dtype.names\n",
    "}\n",
    "df_lr = pd.DataFrame(lr_scores).T\n",
    "\n",
    "# B) Scanpy Wilcoxon scores\n",
    "wl_scores = {\n",
    "    cl: pd.Series(adata_pre.uns['wilcoxon']['scores'][cl],\n",
    "                  index=adata_pre.uns['wilcoxon']['names'][cl])\n",
    "    for cl in adata_pre.uns['wilcoxon']['scores'].dtype.names\n",
    "}\n",
    "df_wl = pd.DataFrame(wl_scores).T\n",
    "\n",
    "# C) Your model’s raw weights\n",
    "df_ml = pd.DataFrame(\n",
    "    adata_model.layers['weights'],\n",
    "    index=adata_model.obs_names,\n",
    "    columns=adata_model.var_names\n",
    ")\n",
    "\n",
    "# ─── 2) Restrict to shared genes and cell lines ────────────────────────────────\n",
    "common_genes = sorted(set(df_lr.columns) & set(df_wl.columns) & set(df_ml.columns))\n",
    "common_cells = sorted(set(df_lr.index)   & set(df_wl.index)   & set(df_ml.index))\n",
    "\n",
    "df_lr = df_lr.loc[common_cells, common_genes]\n",
    "df_wl = df_wl.loc[common_cells, common_genes]\n",
    "df_ml = df_ml.loc[common_cells, common_genes]\n",
    "\n",
    "# ─── 3) Identify top-N genes by absolute score per cell line ──────────────────\n",
    "N = 50\n",
    "top_lr = {cl: df_lr.loc[cl].abs().nlargest(N).index.tolist() for cl in common_cells}\n",
    "top_wl = {cl: df_wl.loc[cl].abs().nlargest(N).index.tolist() for cl in common_cells}\n",
    "top_ml = {cl: df_ml.loc[cl].abs().nlargest(N).index.tolist() for cl in common_cells}\n",
    "\n",
    "# ─── 4) Compute overlap and Spearman correlations ─────────────────────────────\n",
    "records = []\n",
    "for cl in common_cells:\n",
    "    set_lr, set_wl, set_ml = set(top_lr[cl]), set(top_wl[cl]), set(top_ml[cl])\n",
    "    overlap_lr_ml = len(set_lr & set_ml)\n",
    "    overlap_lr_wl = len(set_lr & set_wl)\n",
    "    rho_lr_ml     = spearmanr(df_lr.loc[cl, common_genes],\n",
    "                              df_ml.loc[cl, common_genes]).correlation\n",
    "    rho_lr_wl     = spearmanr(df_lr.loc[cl, common_genes],\n",
    "                              df_wl.loc[cl, common_genes]).correlation\n",
    "    records.append({\n",
    "        'cell_line': cl,\n",
    "        'overlap_logreg_modlyn':     overlap_lr_ml,\n",
    "        'overlap_logreg_wilcox':     overlap_lr_wl,\n",
    "        'spearman_logreg_modlyn':    rho_lr_ml,\n",
    "        'spearman_logreg_wilcox':    rho_lr_wl\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(records).set_index('cell_line')\n",
    "# print(comparison_df)\n",
    "\n",
    "# ─── 5) (Optional) Save or plot comparison_df as needed ───────────────────────\n",
    "# comparison_df.to_csv('method_comparison_summary.csv')\n",
    "\n",
    "# comparison_df has columns:\n",
    "#   ['overlap_logreg_modlyn', 'overlap_logreg_wilcox',\n",
    "#    'spearman_logreg_modlyn', 'spearman_logreg_wilcox']\n",
    "\n",
    "# 1) Split metrics\n",
    "overlap_df = comparison_df[['overlap_logreg_modlyn', 'overlap_logreg_wilcox']]\n",
    "rho_df     = comparison_df[['spearman_logreg_modlyn', 'spearman_logreg_wilcox']]\n",
    "\n",
    "# 2) Set up subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# 3) Heatmap of overlaps\n",
    "sns.heatmap(\n",
    "    overlap_df,\n",
    "    annot=True, fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar_kws={\"label\": \"Number of shared top-50 genes\"},\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Overlap of Top-50 Gene Sets\")\n",
    "axes[0].set_xlabel(\"Comparison\")\n",
    "axes[0].set_ylabel(\"Cell Line\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4) Heatmap of Spearman correlations\n",
    "sns.heatmap(\n",
    "    rho_df,\n",
    "    annot=True, fmt=\".2f\",\n",
    "    cmap=\"vlag\",\n",
    "    center=0,\n",
    "    cbar_kws={\"label\": \"Spearman ρ\"},\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Spearman Correlation of Full Rankings\")\n",
    "axes[1].set_xlabel(\"Comparison\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913b34b-f894-409c-aee9-f0c0c0794ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ─── 0) Prepare effect & confidence matrices for each method ───────────────────\n",
    "# df_lr: Scanpy logreg scores    (adata_pre.uns['logreg']['scores'])\n",
    "# df_wl: Scanpy Wilcoxon scores  (adata_pre.uns['wilcoxon']['scores'])\n",
    "# df_ml: Modlyn weights          (adata_model.layers['weights'])\n",
    "\n",
    "# Align on shared genes/cell_lines\n",
    "common_genes = sorted(set(df_lr.columns) & set(df_wl.columns) & set(df_ml.columns))\n",
    "common_cells = sorted(set(df_lr.index)   & set(df_wl.index)   & set(df_ml.index))\n",
    "df_lr = df_lr.loc[common_cells, common_genes]\n",
    "df_wl = df_wl.loc[common_cells, common_genes]\n",
    "df_ml = df_ml.loc[common_cells, common_genes]\n",
    "\n",
    "# Define confidence proxies (absolute scores)  \n",
    "conf_lr = df_lr.abs()   # logreg z-scores ∝ −log₁₀(p) :contentReference[oaicite:0]{index=0}  \n",
    "conf_wl = df_wl.abs()   # Wilcoxon U-statistics :contentReference[oaicite:1]{index=1}  \n",
    "conf_ml = df_ml.abs()   # log-odds (weights) ∝ effect strength :contentReference[oaicite:2]{index=2}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c5958-a6bb-40d3-99c9-0f43cd1627a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) Build significance sets over confidence thresholds ─────────────────────\n",
    "def build_sigsets(df, conf, effect_thresh):\n",
    "    \"\"\"\n",
    "    For a method:\n",
    "      • df: (cells×genes) effect sizes  \n",
    "      • conf: same shape, confidence measure  \n",
    "      • effect_thresh: scalar threshold on abs(effect)  \n",
    "    Returns: sorted list of (threshold, set of (cell,gene))\n",
    "    \"\"\"\n",
    "    # 1a) mask low-effect entries  \n",
    "    mask = df.abs() >= effect_thresh  # effect filtering :contentReference[oaicite:3]{index=3}  \n",
    "    df_eff = df.where(mask).stack()    # stack to Series of ((cell,gene)->value) :contentReference[oaicite:4]{index=4}  \n",
    "    conf_eff = conf.stack()            # same structure  \n",
    "\n",
    "    # 1b) unique confidence cutoffs (percentiles)  \n",
    "    cuts = np.unique(conf_eff.values)\n",
    "    cuts.sort()\n",
    "\n",
    "    sigsets = []\n",
    "    for c in cuts:\n",
    "        sel = df_eff[conf_eff>=c]\n",
    "        pairs = set(zip(sel.index.get_level_values(0),\n",
    "                        sel.index.get_level_values(1)))\n",
    "        sigsets.append((c, pairs))\n",
    "    return sigsets\n",
    "\n",
    "# thresholds for effect-size filtering\n",
    "# effect_thresh = {\n",
    "#     'logreg':   1.96,        # only genes with |z| ≥1.96 (~p<0.05)  \n",
    "#     'wilcox':   1.96,        # same for Wilcoxon U test z-score  \n",
    "#     'modlyn':   np.log(4)    # weights ≥log(4) ≈ 2× fold-change in odds  \n",
    "# }\n",
    "\n",
    "\n",
    "p = 60  # top 10%\n",
    "effect_thresh = {\n",
    "    'logreg': np.percentile(df_lr.abs().values.flatten(),     p),\n",
    "    'wilcox': np.percentile(df_wl.abs().values.flatten(),     p),\n",
    "    'modlyn': np.percentile(df_ml.abs().values.flatten(),     p),\n",
    "}\n",
    "\n",
    "\n",
    "sig_lr = build_sigsets(df_lr, conf_lr, effect_thresh['logreg'])\n",
    "sig_wl = build_sigsets(df_wl, conf_wl, effect_thresh['wilcox'])\n",
    "sig_ml = build_sigsets(df_ml, conf_ml, effect_thresh['modlyn'])\n",
    "\n",
    "# ─── 2) Compute PR curves & AUPR ────────────────────────────────────────────────\n",
    "def pr_auc(truth_sets, test_sets):\n",
    "    \"\"\"\n",
    "    truth_sets: list of (thr, set) from most liberal to conservative  \n",
    "    test_sets: same  \n",
    "    \"\"\"\n",
    "    # ground truth = liberalest set  \n",
    "    gt = truth_sets[0][1]  \n",
    "    precisions, recalls = [], []\n",
    "    for _, test in test_sets:\n",
    "        tp = len(gt & test)\n",
    "        fp = len(test) - tp\n",
    "        fn = len(gt) - tp\n",
    "        p = tp/(tp+fp) if tp+fp>0 else 1.0  # precision definition :contentReference[oaicite:6]{index=6}  \n",
    "        r = tp/(tp+fn) if tp+fn>0 else 0.0  # recall definition :contentReference[oaicite:7]{index=7}  \n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "    return np.array(recalls), np.array(precisions), auc(recalls, precisions)\n",
    "\n",
    "r_wl, p_wl, auc_wl = pr_auc(sig_lr, sig_wl)  # Wilcoxon vs logreg  \n",
    "r_ml, p_ml, auc_ml = pr_auc(sig_lr, sig_ml)  # Modlyn vs logreg  \n",
    "\n",
    "# ─── 3) Plot Precision–Recall curves ───────────────────────────────────────────\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(r_wl, p_wl, label=f'Wilcoxon vs LogReg  (AUPR={auc_wl:.3f})')\n",
    "plt.plot(r_ml, p_ml, label=f'Modlyn vs LogReg    (AUPR={auc_ml:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision–Recall Comparison') \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa346a91-c10f-4ed3-b3c5-3c521e31e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1bc7d-176d-4f2a-abe6-2b2f1f8a0ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d545f-6050-4f1c-979c-44b6f291eaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2e782-ea3a-4f1e-abde-8c2373bc5211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamin_env",
   "language": "python",
   "name": "lamin_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
