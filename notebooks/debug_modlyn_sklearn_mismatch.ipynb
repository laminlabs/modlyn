{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Debugging Modlyn vs Sklearn Mismatch\n",
    "\n",
    "**Problem**: Modlyn and Sklearn are giving very different results:\n",
    "- Mean weight correlation: -0.034 (essentially no correlation)\n",
    "- Training accuracies: Modlyn 0.04 vs Sklearn 0.16\n",
    "- 0/39 cell lines with >99% correlation\n",
    "\n",
    "**Potential causes from Alex Wolf's insights**:\n",
    "1. **Regularization mismatch**: sklearn has default L2, modlyn might not match\n",
    "2. **Optimizer differences**: sklearn uses LBFGS, Lightning uses Adam/SGD\n",
    "3. **Training differences**: epochs, convergence criteria\n",
    "4. **Data preprocessing**: normalization, scaling differences\n",
    "5. **Model architecture**: linear layer equivalence\n",
    "\n",
    "**Systematic debugging approach**:\n",
    "1. Check data preprocessing consistency\n",
    "2. Match regularization parameters exactly\n",
    "3. Use identical optimizers and training procedures\n",
    "4. Verify model architecture equivalence\n",
    "5. Test on simple synthetic data first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import torch\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modlyn.models import SimpleLogReg, SimpleLogRegDataModule\n",
    "import lamindb as ln\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Create Simple Test Data\n",
    "\n",
    "Start with synthetic data where we know the ground truth to isolate the model differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple synthetic data with known structure\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "\n",
    "# Create linearly separable data\n",
    "np.random.seed(42)\n",
    "X_synthetic = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create clear linear decision boundaries\n",
    "true_weights = np.random.randn(n_classes, n_features)\n",
    "true_bias = np.random.randn(n_classes)\n",
    "\n",
    "# Generate labels based on linear model + noise\n",
    "scores = X_synthetic @ true_weights.T + true_bias\n",
    "y_synthetic = np.argmax(scores, axis=1)\n",
    "\n",
    "print(f\"Synthetic data shape: {X_synthetic.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_synthetic)}\")\n",
    "print(f\"True weights shape: {true_weights.shape}\")\n",
    "\n",
    "# Create AnnData object\n",
    "adata_synthetic = ad.AnnData(X=X_synthetic)\n",
    "adata_synthetic.obs['y'] = y_synthetic\n",
    "adata_synthetic.obs['cell_line'] = [f'class_{i}' for i in y_synthetic]\n",
    "adata_synthetic.var_names = [f'feature_{i}' for i in range(n_features)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Test Sklearn with Different Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data identically for both methods\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_synthetic, y_synthetic, test_size=0.2, random_state=42, stratify=y_synthetic\n",
    ")\n",
    "\n",
    "print(\"=== SKLEARN LOGISTIC REGRESSION ANALYSIS ===\")\n",
    "\n",
    "# Default sklearn (with L2 regularization)\n",
    "sklearn_default = LogisticRegression(random_state=42, max_iter=1000)\n",
    "sklearn_default.fit(X_train, y_train)\n",
    "acc_default = sklearn_default.score(X_train, y_train)\n",
    "print(f\"Sklearn (default L2): Train accuracy = {acc_default:.4f}\")\n",
    "\n",
    "# No regularization\n",
    "sklearn_no_reg = LogisticRegression(C=1e10, random_state=42, max_iter=1000)  # Very high C = low regularization\n",
    "sklearn_no_reg.fit(X_train, y_train)\n",
    "acc_no_reg = sklearn_no_reg.score(X_train, y_train)\n",
    "print(f\"Sklearn (no regularization): Train accuracy = {acc_no_reg:.4f}\")\n",
    "\n",
    "# High regularization\n",
    "sklearn_high_reg = LogisticRegression(C=0.01, random_state=42, max_iter=1000)  # Low C = high regularization\n",
    "sklearn_high_reg.fit(X_train, y_train)\n",
    "acc_high_reg = sklearn_high_reg.score(X_train, y_train)\n",
    "print(f\"Sklearn (high regularization): Train accuracy = {acc_high_reg:.4f}\")\n",
    "\n",
    "print(f\"\\nSklearn default parameters:\")\n",
    "print(f\"C (inverse regularization): {sklearn_default.C}\")\n",
    "print(f\"Penalty: {sklearn_default.penalty}\")\n",
    "print(f\"Solver: {sklearn_default.solver}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Test Modlyn with Matched Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODLYN ANALYSIS ===\")\n",
    "\n",
    "# Create identical training data for modlyn\n",
    "adata_train = ad.AnnData(X=X_train)\n",
    "adata_train.obs['y'] = y_train\n",
    "adata_train.obs['cell_line'] = [f'class_{i}' for i in y_train]\n",
    "adata_train.var_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "adata_val = ad.AnnData(X=X_val)\n",
    "adata_val.obs['y'] = y_val\n",
    "adata_val.obs['cell_line'] = [f'class_{i}' for i in y_val]\n",
    "adata_val.var_names = [f'feature_{i}' for i in range(X_val.shape[1])]\n",
    "\n",
    "# Test different weight_decay values to match sklearn's regularization\n",
    "# sklearn C=1.0 (default) roughly corresponds to weight_decay = 1/C = 1.0\n",
    "weight_decay_values = [0.0, 0.01, 1.0, 100.0]  # Test range from no reg to high reg\n",
    "\n",
    "modlyn_results = {}\n",
    "\n",
    "for wd in weight_decay_values:\n",
    "    print(f\"\\nTesting weight_decay = {wd}\")\n",
    "    \n",
    "    # Create datamodule\n",
    "    datamodule = SimpleLogRegDataModule(\n",
    "        adata_train=adata_train,\n",
    "        adata_val=adata_val,\n",
    "        label_column=\"y\",\n",
    "        train_dataloader_kwargs={\"batch_size\": len(adata_train), \"num_workers\": 0},  # Full batch\n",
    "        val_dataloader_kwargs={\"batch_size\": len(adata_val), \"num_workers\": 0}\n",
    "    )\n",
    "    \n",
    "    # Create model with specific weight_decay\n",
    "    model = SimpleLogReg(\n",
    "        adata=adata_train,\n",
    "        label_column=\"y\",\n",
    "        learning_rate=1e-2,  # Start with a reasonable learning rate\n",
    "        weight_decay=wd\n",
    "    )\n",
    "    \n",
    "    # Train with more epochs to ensure convergence\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=100,  # More epochs for convergence\n",
    "        enable_progress_bar=False,\n",
    "        logger=False,\n",
    "        enable_checkpointing=False\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "    # Get predictions and accuracy\n",
    "    with torch.no_grad():\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        predictions = model(X_train_tensor)\n",
    "        predicted_classes = predictions.argmax(dim=1).numpy()\n",
    "        accuracy = (predicted_classes == y_train).mean()\n",
    "    \n",
    "    print(f\"  Train accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    weights = model.linear.weight.detach().cpu().numpy()\n",
    "    modlyn_results[wd] = {\n",
    "        'accuracy': accuracy,\n",
    "        'weights': weights,\n",
    "        'model': model\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Compare Weight Correlations and Recommend Fixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== WEIGHT CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Compare modlyn weights with sklearn weights\n",
    "sklearn_weights_default = sklearn_default.coef_  # Shape: (n_classes, n_features)\n",
    "sklearn_weights_no_reg = sklearn_no_reg.coef_\n",
    "\n",
    "correlations = {}\n",
    "\n",
    "for wd, results in modlyn_results.items():\n",
    "    modlyn_weights = results['weights']  # Shape: (n_classes, n_features)\n",
    "    \n",
    "    # Calculate correlation between flattened weight matrices\n",
    "    corr_default = np.corrcoef(modlyn_weights.flatten(), sklearn_weights_default.flatten())[0, 1]\n",
    "    corr_no_reg = np.corrcoef(modlyn_weights.flatten(), sklearn_weights_no_reg.flatten())[0, 1]\n",
    "    \n",
    "    correlations[wd] = {\n",
    "        'vs_sklearn_default': corr_default,\n",
    "        'vs_sklearn_no_reg': corr_no_reg,\n",
    "        'modlyn_accuracy': results['accuracy']\n",
    "    }\n",
    "    \n",
    "    print(f\"Weight_decay {wd}:\")\n",
    "    print(f\"  vs sklearn default: {corr_default:.4f}\")\n",
    "    print(f\"  vs sklearn no reg: {corr_no_reg:.4f}\")\n",
    "    print(f\"  modlyn accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nSklearn accuracies for reference:\")\n",
    "print(f\"  Default: {acc_default:.4f}\")\n",
    "print(f\"  No reg: {acc_no_reg:.4f}\")\n",
    "print(f\"  High reg: {acc_high_reg:.4f}\")\n",
    "\n",
    "# Find the best matching configuration\n",
    "best_wd = max(correlations.keys(), key=lambda x: correlations[x]['vs_sklearn_default'])\n",
    "best_corr = correlations[best_wd]['vs_sklearn_default']\n",
    "\n",
    "print(f\"\\nüéØ BEST MATCH FOUND:\")\n",
    "print(f\"   weight_decay = {best_wd}\")\n",
    "print(f\"   correlation = {best_corr:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "wd_values = list(correlations.keys())\n",
    "modlyn_accs = [correlations[wd]['modlyn_accuracy'] for wd in wd_values]\n",
    "\n",
    "axes[0].plot(wd_values, modlyn_accs, 'o-', label='Modlyn', linewidth=2)\n",
    "axes[0].axhline(acc_default, color='red', linestyle='--', label='Sklearn default')\n",
    "axes[0].axhline(acc_no_reg, color='orange', linestyle='--', label='Sklearn no reg')\n",
    "axes[0].set_xlabel('Weight Decay')\n",
    "axes[0].set_ylabel('Training Accuracy')\n",
    "axes[0].set_title('Accuracy vs Regularization')\n",
    "axes[0].set_xscale('symlog')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 2. Correlation vs weight decay\n",
    "corr_values = [correlations[wd]['vs_sklearn_default'] for wd in wd_values]\n",
    "\n",
    "axes[1].plot(wd_values, corr_values, 'o-', color='green', linewidth=2)\n",
    "axes[1].axhline(0.95, color='red', linestyle='--', alpha=0.7, label='Good match (>0.95)')\n",
    "axes[1].set_xlabel('Weight Decay')\n",
    "axes[1].set_ylabel('Weight Correlation with Sklearn')\n",
    "axes[1].set_title('Correlation vs Regularization')\n",
    "axes[1].set_xscale('symlog')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS FOR YOUR VALIDATION NOTEBOOK ===\")\n",
    "print(f\"Replace in your SimpleLogReg creation:\")\n",
    "print(f\"  weight_decay={best_wd}  # (current issue: likely too low or zero)\")\n",
    "print(f\"  learning_rate=1e-2\")\n",
    "print(f\"  max_epochs=100  # (current issue: likely too few epochs)\")\n",
    "print(f\"  batch_size=len(adata_train)  # (use full batch for small datasets)\")\n",
    "\n",
    "if best_corr > 0.95:\n",
    "    print(f\"\\n‚úÖ Expected improvement: EXCELLENT match (correlation = {best_corr:.3f})\")\n",
    "elif best_corr > 0.8:\n",
    "    print(f\"\\n‚ö†Ô∏è  Expected improvement: GOOD match (correlation = {best_corr:.3f})\")\n",
    "    print(f\"   May need additional tuning\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Expected improvement: LIMITED (correlation = {best_corr:.3f})\")\n",
    "    print(f\"   May need to investigate other factors\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
