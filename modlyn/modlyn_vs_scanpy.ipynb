{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a0f5f-4d28-4c3b-9a81-30e407538ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import lamindb as ln\n",
    "from modlyn.io.loading import read_lazy\n",
    "from modlyn.io.datamodules import ClassificationDataModule\n",
    "from modlyn.models.linear import Linear\n",
    "import lightning as L\n",
    "\n",
    "# Set up scanpy settings\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=80, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac27960-562b-4564-9f6c-9d5187a25ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: Create Clean 100k Subset (Reproducible)\n",
    "# =============================================================================\n",
    "\n",
    "# def create_clean_subset(store_path, var_file, n_cells=100000, seed=42):\n",
    "#     \"\"\"Create a clean, balanced subset of the data\"\"\"\n",
    "#     print(\"Loading full dataset...\")\n",
    "#     adata_full = read_lazy(store_path)\n",
    "#     var = pd.read_parquet(var_file)\n",
    "#     adata_full.var = var\n",
    "    \n",
    "#     np.random.seed(seed)\n",
    "#     cell_lines = adata_full.obs['cell_line'].unique()\n",
    "#     n_per_line = n_cells // len(cell_lines)\n",
    "    \n",
    "#     print(f\"Creating subset with {n_per_line} cells per line from {len(cell_lines)} cell lines\")\n",
    "    \n",
    "#     subset_indices = []\n",
    "#     for cell_line in cell_lines:\n",
    "#         mask = adata_full.obs['cell_line'] == cell_line\n",
    "#         indices = np.where(mask)[0]\n",
    "#         if len(indices) >= n_per_line:\n",
    "#             selected = np.random.choice(indices, n_per_line, replace=False)\n",
    "#             subset_indices.extend(selected)\n",
    "    \n",
    "#     adata = adata_full[subset_indices].copy()\n",
    "#     print(f\"Clean subset: {adata.n_obs} cells, {adata.n_vars} genes\")\n",
    "#     print(f\"Cell lines: {adata.obs['cell_line'].value_counts()}\")\n",
    "    \n",
    "#     return adata\n",
    "\n",
    "# # Create the subset\n",
    "# store_path = Path(\"/home/ubuntu/tahoe100M_chunk_1\")\n",
    "# adata = create_clean_subset(store_path, \"var_subset_tahoe100M.parquet\", n_cells=1000)\n",
    "# adata\n",
    "\n",
    "\n",
    "def create_clean_subset(store_path, var_file, n_cells=100000, n_genes=5000, seed=42):\n",
    "    \"\"\"Create a clean, balanced subset of the data with random gene selection\"\"\"\n",
    "    print(\"Loading full dataset...\")\n",
    "    adata_full = read_lazy(store_path)\n",
    "    var = pd.read_parquet(var_file)\n",
    "    adata_full.var = var\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # First subset cells\n",
    "    cell_lines = adata_full.obs['cell_line'].unique()\n",
    "    n_per_line = n_cells // len(cell_lines)\n",
    "    \n",
    "    print(f\"Creating subset with {n_per_line} cells per line from {len(cell_lines)} cell lines\")\n",
    "    \n",
    "    subset_indices = []\n",
    "    for cell_line in cell_lines:\n",
    "        mask = adata_full.obs['cell_line'] == cell_line\n",
    "        indices = np.where(mask)[0]\n",
    "        if len(indices) >= n_per_line:\n",
    "            selected = np.random.choice(indices, n_per_line, replace=False)\n",
    "            subset_indices.extend(selected)\n",
    "    \n",
    "    adata = adata_full[subset_indices].copy()\n",
    "    \n",
    "    # Random gene subset\n",
    "    print(f\"Randomly selecting {n_genes} genes...\")\n",
    "    gene_indices = np.random.choice(adata.n_vars, size=min(n_genes, adata.n_vars), replace=False)\n",
    "    adata = adata[:, gene_indices].copy()\n",
    "    \n",
    "    print(f\"Clean subset: {adata.n_obs} cells, {adata.n_vars} genes\")\n",
    "    print(f\"Cell lines: {adata.obs['cell_line'].value_counts()}\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "# Create the subset\n",
    "store_path = Path(\"/home/ubuntu/tahoe100M_chunk_1\")\n",
    "adata = create_clean_subset(store_path, \"var_subset_tahoe100M.parquet\", n_cells=1000, n_genes=5000)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedded3-a2ed-4767-a004-36d92d78b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Scanpy Differential Expression\n",
    "# =============================================================================\n",
    "\n",
    "def run_scanpy_analysis(adata, n_genes=50):\n",
    "    \"\"\"Run scanpy differential expression analysis\"\"\"\n",
    "    print(\"Running Scanpy analysis...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    adata_scanpy = adata.copy()\n",
    "    adata_scanpy.X = adata_scanpy.X.compute()\n",
    "    if hasattr(adata_scanpy.X, 'toarray'):\n",
    "        adata_scanpy.X = adata_scanpy.X.toarray()\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    sc.pp.normalize_total(adata_scanpy, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_scanpy)\n",
    "    \n",
    "    # Run differential expression\n",
    "    sc.tl.rank_genes_groups(adata_scanpy, 'cell_line', method='wilcoxon', n_genes=n_genes)\n",
    "    \n",
    "    scanpy_results = {}\n",
    "    for cell_line in adata_scanpy.obs['cell_line'].cat.categories:\n",
    "        genes_df = sc.get.rank_genes_groups_df(adata_scanpy, group=cell_line)\n",
    "        scanpy_results[cell_line] = genes_df.set_index('names')\n",
    "    \n",
    "    return adata_scanpy, scanpy_results\n",
    "\n",
    "adata_scanpy, scanpy_results = run_scanpy_analysis(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdb31a-aa6f-4cd9-ad8a-8ace2f5e6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Modlyn\n",
    "# =============================================================================\n",
    "\n",
    "def run_modlyn_analysis(adata, n_epochs=5, n_genes=50):\n",
    "    \"\"\"Run modlyn logistic regression analysis\"\"\"\n",
    "    print(\"Running Modlyn analysis...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    modlyn_adata = adata.copy()\n",
    "    modlyn_adata.obs[\"y\"] = modlyn_adata.obs[\"cell_line\"].astype(\"category\").cat.codes.to_numpy().astype(\"i8\")\n",
    "    \n",
    "    # Train/val split\n",
    "    n_train = int(0.8 * modlyn_adata.n_obs)\n",
    "    adata_train = modlyn_adata[:n_train]\n",
    "    adata_val = modlyn_adata[n_train:]\n",
    "    \n",
    "    # Create datamodule\n",
    "    datamodule = ClassificationDataModule(\n",
    "        adata_train=adata_train,\n",
    "        adata_val=adata_val,\n",
    "        label_column=\"y\",\n",
    "        train_dataloader_kwargs={\"batch_size\": 1024, \"drop_last\": True},\n",
    "        val_dataloader_kwargs={\"batch_size\": 1024, \"drop_last\": False},\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    linear = Linear(\n",
    "        n_genes=modlyn_adata.n_vars,\n",
    "        n_covariates=modlyn_adata.obs[\"y\"].nunique(),\n",
    "        learning_rate=1e-3,\n",
    "    )\n",
    "    \n",
    "    trainer = L.Trainer(max_epochs=n_epochs, log_every_n_steps=50, enable_progress_bar=True)\n",
    "    trainer.fit(linear, datamodule)\n",
    "    \n",
    "    weights = linear.linear.weight.detach().cpu().numpy()\n",
    "    cell_line_names = modlyn_adata.obs['cell_line'].cat.categories\n",
    "    \n",
    "    modlyn_results = {}\n",
    "    for i, cell_line in enumerate(cell_line_names):\n",
    "        weights_series = pd.Series(weights[i], index=modlyn_adata.var_names)\n",
    "        top_genes = weights_series.abs().nlargest(n_genes)\n",
    "        \n",
    "        modlyn_results[cell_line] = pd.DataFrame({\n",
    "            'weight': weights_series[top_genes.index],\n",
    "            'abs_weight': top_genes,\n",
    "            'logfoldchange': weights_series[top_genes.index],  # For compatibility\n",
    "            'pvals_adj': np.ones(len(top_genes)) * 0.05,  # Placeholder\n",
    "        })\n",
    "    \n",
    "    return modlyn_adata, modlyn_results\n",
    "\n",
    "modlyn_adata, modlyn_results = run_modlyn_analysis(adata, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25c8c4-a08d-4a7d-96eb-a559863ea668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative_dotplots(adata_scanpy, modlyn_adata, scanpy_results, modlyn_results, \n",
    "                            top_n=10, figsize=(16, 6)):\n",
    "    \"\"\"Simple side-by-side dotplots\"\"\"\n",
    "    \n",
    "    # Just get top genes from scanpy results\n",
    "    all_top_genes = set()\n",
    "    for cell_line, df in scanpy_results.items():\n",
    "        all_top_genes.update(df.head(top_n).index.tolist())\n",
    "    \n",
    "    # Limit to reasonable number for visualization\n",
    "    plot_genes = list(all_top_genes)[:20]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Scanpy dotplot\n",
    "    sc.pl.dotplot(adata_scanpy, plot_genes, groupby='cell_line', \n",
    "                  ax=ax1, show=False, dendrogram=False)\n",
    "    ax1.set_title('Scanpy (Wilcoxon)')\n",
    "    \n",
    "    print(\"---\")\n",
    "    # Modlyn dotplot - just normalize the same way\n",
    "    modlyn_adata_simple = modlyn_adata.copy()\n",
    "    modlyn_adata_simple.X = modlyn_adata_simple.X.compute()\n",
    "    \n",
    "    if hasattr(modlyn_adata_simple.X, 'toarray'):\n",
    "        modlyn_adata_simple.X = modlyn_adata_simple.X.toarray()\n",
    "    sc.pp.normalize_total(modlyn_adata_simple, target_sum=1e4)\n",
    "    sc.pp.log1p(modlyn_adata_simple)\n",
    "    \n",
    "    sc.pl.dotplot(modlyn_adata_simple, plot_genes, groupby='cell_line', \n",
    "                  ax=ax2, show=False, dendrogram=False)\n",
    "    ax2.set_title('Modlyn (Logistic Regression)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_comparative_dotplots(adata_scanpy, modlyn_adata, scanpy_results, modlyn_results, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968cc1f-bb5b-4010-bb1c-6c5f80acd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank_correlations(scanpy_results, modlyn_results, top_n=20):\n",
    "    \"\"\"Simple rank correlation calculation\"\"\"\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    for cell_line in scanpy_results.keys():\n",
    "        if cell_line in modlyn_results:\n",
    "            # Get top genes from each method\n",
    "            scanpy_genes = scanpy_results[cell_line].head(top_n).index.tolist()\n",
    "            modlyn_genes = modlyn_results[cell_line].head(top_n).index.tolist()\n",
    "            \n",
    "            # Find common genes\n",
    "            common_genes = list(set(scanpy_genes) & set(modlyn_genes))\n",
    "            \n",
    "            if len(common_genes) > 3:  # Need minimum overlap\n",
    "                # Get ranks for common genes\n",
    "                scanpy_ranks = [scanpy_genes.index(gene) for gene in common_genes]\n",
    "                modlyn_ranks = [modlyn_genes.index(gene) for gene in common_genes]\n",
    "                \n",
    "                # Calculate correlation\n",
    "                spearman_r, _ = spearmanr(scanpy_ranks, modlyn_ranks)\n",
    "                \n",
    "                correlations[cell_line] = {\n",
    "                    'n_common': len(common_genes),\n",
    "                    'correlation': spearman_r,\n",
    "                    'scanpy_ranks': scanpy_ranks,\n",
    "                    'modlyn_ranks': modlyn_ranks\n",
    "                }\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def plot_rank_correlations(correlations):\n",
    "    \"\"\"Simple correlation plot\"\"\"\n",
    "    \n",
    "    if not correlations:\n",
    "        print(\"No correlations to plot - no overlapping genes found\")\n",
    "        return None\n",
    "    \n",
    "    n_lines = len(correlations)\n",
    "    n_cols = min(4, n_lines)\n",
    "    n_rows = max(1, (n_lines + n_cols - 1) // n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 3*n_rows))\n",
    "    if n_lines == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes if n_cols > 1 else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (cell_line, data) in enumerate(correlations.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        ax.scatter(data['scanpy_ranks'], data['modlyn_ranks'], alpha=0.7)\n",
    "        ax.set_title(f'{cell_line}\\nr = {data[\"correlation\"]:.2f}')\n",
    "        ax.set_xlabel('Scanpy Rank')\n",
    "        ax.set_ylabel('Modlyn Rank')\n",
    "        \n",
    "        # Add diagonal line\n",
    "        max_rank = max(max(data['scanpy_ranks']), max(data['modlyn_ranks']))\n",
    "        ax.plot([0, max_rank], [0, max_rank], 'r--', alpha=0.5)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for i in range(len(correlations), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def print_summary_statistics(correlations):\n",
    "    \"\"\"Simple summary\"\"\"\n",
    "    \n",
    "    print(\"SCANPY vs MODLYN COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    correlations_list = [c['correlation'] for c in correlations.values() if not np.isnan(c['correlation'])]\n",
    "    n_common_list = [c['n_common'] for c in correlations.values()]\n",
    "    \n",
    "    print(f\"Cell lines analyzed: {len(correlations)}\")\n",
    "    print(f\"Average overlap: {np.mean(n_common_list):.1f} genes\")\n",
    "    print(f\"Average correlation: {np.mean(correlations_list):.3f}\")\n",
    "    \n",
    "    print(\"\\nBy cell line:\")\n",
    "    for cell_line, data in correlations.items():\n",
    "        print(f\"{cell_line}: {data['n_common']} genes, r = {data['correlation']:.3f}\")\n",
    "\n",
    "# Run analysis\n",
    "correlations = calculate_rank_correlations(scanpy_results, modlyn_results, top_n=20)\n",
    "fig_corr = plot_rank_correlations(correlations)\n",
    "plt.show()\n",
    "print_summary_statistics(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4facf-829a-4d83-bcc7-14496200f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 7: Save Results\n",
    "# =============================================================================\n",
    "\n",
    "# Save the clean subset for future use\n",
    "print(\"\\nSaving clean subset...\")\n",
    "adata.write_h5ad('clean_subset_100k.h5ad')\n",
    "\n",
    "# Save results\n",
    "import pickle\n",
    "with open('comparison_results.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'scanpy_results': scanpy_results,\n",
    "        'modlyn_results': modlyn_results,\n",
    "        'correlations': correlations\n",
    "    }, f)\n",
    "\n",
    "print(\"Analysis complete! Files saved:\")\n",
    "print(\"- clean_subset_100k.h5ad\")\n",
    "print(\"- comparison_results.pkl\") \n",
    "print(\"- comparative_dotplots.png\")\n",
    "print(\"- rank_correlations.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamin_env",
   "language": "python",
   "name": "lamin_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
